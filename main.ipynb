{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b59b1471-609d-41be-825f-f2a42235af19",
   "metadata": {},
   "source": [
    "### Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9307cc51-be72-4b50-8b91-9dd73816f354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from hampel import hampel\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "C_ROOT_DIR = 'data/charging/'\n",
    "D_ROOT_DIR = 'data/discharging/'\n",
    "\n",
    "# Charging\n",
    "C_train_df = pd.read_pickle(os.path.join(C_ROOT_DIR, f'train_df.pkl'))\n",
    "C_val_df = pd.read_pickle(os.path.join(C_ROOT_DIR, f'val_df.pkl'))\n",
    "C_test_df = pd.read_pickle(os.path.join(C_ROOT_DIR, f'test_df.pkl'))\n",
    "\n",
    "# Discharging\n",
    "D_train_df = pd.read_pickle(os.path.join(D_ROOT_DIR, f'train_df.pkl'))\n",
    "D_val_df = pd.read_pickle(os.path.join(D_ROOT_DIR, f'val_df.pkl'))\n",
    "D_test_df = pd.read_pickle(os.path.join(D_ROOT_DIR, f'test_df.pkl'))\n",
    "\n",
    "\n",
    "# Standardize and extract data\n",
    "features = ['I', 'V', 'T', 'c', 'IR', 'dV/dt']\n",
    "label = 'Q'\n",
    "\n",
    "C_X_train = StandardScaler().fit_transform(C_train_df[features])\n",
    "C_y_train = C_train_df[[label]].to_numpy()\n",
    "\n",
    "D_X_train = StandardScaler().fit_transform(D_train_df[features])\n",
    "D_y_train = D_train_df[[label]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c537b57-42e4-437d-ae11-045db13c7958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign GPU if available \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be3a323-5591-46d5-ba35-441647db3642",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5887799-a36e-4ca1-83d3-565cdd7ae6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_units = 50\n",
    "dropout = 0.2547\n",
    "class charging_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(6, num_hidden_units).float()\n",
    "        self.act1 = nn.Sigmoid() \n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "        self.layer2 = nn.Linear(num_hidden_units, num_hidden_units).float()\n",
    "        self.act2 = nn.Sigmoid() \n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.layer3 = nn.Linear(num_hidden_units, num_hidden_units).float()\n",
    "        self.act3 = nn.Sigmoid() \n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.layer4 = nn.Linear(num_hidden_units, num_hidden_units).float()\n",
    "        self.act4 = nn.Sigmoid()\n",
    "   \n",
    "\n",
    "        self.output = nn.Linear(num_hidden_units, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.layer1(x))\n",
    "        x = self.dropout1(x)\n",
    "       \n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.act3(self.layer3(x))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.act4(self.layer4(x))\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "C_ff = charging_model().to(device)\n",
    "\n",
    "\n",
    "num_hidden_units = 100\n",
    "dropout = 0.1545\n",
    "class discharging_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(6, num_hidden_units).float()\n",
    "        self.act1 = nn.Sigmoid() \n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "        self.layer2 = nn.Linear(num_hidden_units, num_hidden_units).float()\n",
    "        self.act2 = nn.Sigmoid() \n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.layer3 = nn.Linear(num_hidden_units, num_hidden_units).float()\n",
    "        self.act3 = nn.Sigmoid() \n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.output = nn.Linear(num_hidden_units, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.layer1(x))\n",
    "        x = self.dropout1(x)\n",
    "       \n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.act3(self.layer3(x))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.output(x)\n",
    "       \n",
    "        return x\n",
    "\n",
    "D_ff = discharging_model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8e3e3b-9386-4636-ad6f-1b1589c396a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_window_size = 19\n",
    "D_window_size = 2\n",
    "\n",
    "num_hidden_units = 200\n",
    "class discharging_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm1 = nn.LSTM(6, num_hidden_units, batch_first=True).float()\n",
    "        self.lstm1.flatten_parameters() \n",
    "\n",
    "\n",
    "        self.layer2 = nn.Linear(num_hidden_units, num_hidden_units).float()\n",
    "        self.act2 = nn.Sigmoid()\n",
    "\n",
    "        self.output = nn.Linear(num_hidden_units, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "        \n",
    "D_lstm = discharging_LSTM().to(device)\n",
    "\n",
    "\n",
    "num_hidden_units = 100\n",
    "class charging_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(6, num_hidden_units, batch_first=True).float()\n",
    "        self.lstm1.flatten_parameters() \n",
    "\n",
    "        self.layer2 = nn.Linear(num_hidden_units, num_hidden_units).float()\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "        self.output = nn.Linear(num_hidden_units, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "C_lstm = charging_LSTM().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de146398-5240-4257-aa78-26bd3a4660c1",
   "metadata": {},
   "source": [
    "### Helper functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5faab4c-cabc-4ba7-990f-b6a5a8e22595",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "# change the data to sequence to sequence data for the lstm\n",
    "def lstm_transform_data(data, target, lookback):\n",
    "    local_data = []\n",
    "    local_target = []\n",
    "    \n",
    "    for i in range(len(data) - lookback):\n",
    "        feature = np.array(data[i: i + lookback])\n",
    "        temp_target = np.array(target[i + 1: i + lookback + 1])\n",
    "\n",
    "        local_data.append(feature)\n",
    "        local_target.append(temp_target)\n",
    "    \n",
    "    X = np.array(local_data)\n",
    "    Y = np.array(local_target)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "\n",
    "# helper function for training\n",
    "def train(model, X_train, y_train, lr=0.01, n_epochs=1, batch_size=128, lookback=None, silent= False):\n",
    "    # LSTM transform\n",
    "    if lookback:\n",
    "        \n",
    "        local_X_train = []\n",
    "        local_y_train = []\n",
    "        for i in range(len(X_train) - lookback):\n",
    "            feature = X_train[i: i + lookback]\n",
    "            target = y_train[i + 1: i + lookback + 1]\n",
    "            local_X_train.append(feature)\n",
    "            local_y_train.append(target)\n",
    "        X_train = np.array(local_X_train)\n",
    "        y_train = np.array(local_y_train)\n",
    "\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    batch_start = torch.arange(0, len(X_train), batch_size)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        for start in tqdm(batch_start, disable=silent):\n",
    "            X_batch = X_train[start:start+batch_size]\n",
    "           \n",
    "            X_batch_tensor = torch.from_numpy(X_batch)\n",
    "            X_batch_tensor = X_batch_tensor.to(device)\n",
    "            X_batch_tensor = X_batch_tensor.float()\n",
    "            y_batch = y_train[start:start+batch_size]\n",
    "            y_batch_tensor = torch.from_numpy(y_batch).to(device).float()\n",
    "            y_pred = model(X_batch_tensor)\n",
    "            loss = loss_fn(y_pred, y_batch_tensor)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36ba8b3-9712-49ff-b0ad-4cb89210a714",
   "metadata": {},
   "source": [
    "### Preload Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad9e758-b992-45ae-9082-ada74bf0df91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use the models used in the paper, load the model state\n",
    "D_trained_lstm = torch.load(\"preTrainedModels/discharging_lstm.pth\")\n",
    "C_trained_lstm = torch.load(\"preTrainedModels/charging_lstm.pth\")\n",
    "C_trained_ff = torch.load(\"preTrainedModels/charging_ff.pth\")       \n",
    "D_trained_ff = torch.load(\"preTrainedModels/discharging_ff.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cee3d4-60a5-4001-a034-bdf56a473a6f",
   "metadata": {},
   "source": [
    "### Optional: Retrain the models to replicate our process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a48406-ed5e-43ee-a17f-806d626cfd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train the models yourself, execute the below code\n",
    "C_trained_ff = train(C_ff, C_X_train, C_y_train, n_epochs=7, batch_size=64, lr=0.005251)\n",
    "D_trained_ff = train(D_ff, D_X_train, D_y_train, n_epochs=7, batch_size=256, lr=0.02625)\n",
    "\n",
    "C_trained_lstm = train(C_lstm, C_X_train, C_y_train, n_epochs=2, batch_size=64, lr=0.01733, lookback=19)\n",
    "D_trained_lstm = train(D_lstm, D_X_train, D_y_train, n_epochs=2, batch_size=128, lr=0.01783, lookback=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319f0882-458b-4953-a928-c8ddfabda97b",
   "metadata": {},
   "source": [
    "### Continuous Learning\n",
    "\n",
    "For simplicity, we seperate true-label fine-tuning from psuedo-label fine-tuning. The former uses the ground truth values for the test set, while the later generates its own labels for the test set (refered to as psuedo-labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05daa7a-4c9b-44a3-b384-30f56a89c8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from hampel import hampel\n",
    "import warnings\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "def generate_pseudo_labels(trained_model, X_unlabeled, batch_size=128, lookback=-1):\n",
    "\n",
    "    if lookback != -1:\n",
    "        X_unlabeled, _ = lstm_transform_data(X_unlabeled, [], lookback)\n",
    "    \n",
    "    batch_start = torch.arange(0, len(X_unlabeled), batch_size)\n",
    "    pseudo_labels = []\n",
    "\n",
    "    trained_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for start in batch_start:\n",
    "            X_batch = X_unlabeled[start:start+batch_size]\n",
    "            X_batch_tensor = torch.from_numpy(X_batch).to(device).float()\n",
    "            \n",
    "            y_pseudo_label = trained_model(X_batch_tensor)\n",
    "            if lookback != -1:\n",
    "                pseudo_labels.append(y_pseudo_label.cpu().numpy()[:, -1, :])\n",
    "            else:\n",
    "                pseudo_labels.append(y_pseudo_label.cpu().numpy())\n",
    "\n",
    "    return np.concatenate(pseudo_labels, axis=0)\n",
    "\n",
    "def rolling_median_filter(data, window_size=100):\n",
    "    data_filtered = data.copy()\n",
    "    n = len(data)\n",
    "    \n",
    "    for i in range(n):\n",
    "        # Define the left and right window boundaries\n",
    "        left = max(0, i - window_size)\n",
    "        right = min(n, i + window_size + 1)\n",
    "        \n",
    "        # Compute the median of the surrounding values\n",
    "        median = np.median(data[left:right])\n",
    "\n",
    "        # Replace value if it's outside the median range\n",
    "        if data[i] > (median * 1.5):\n",
    "            data_filtered[i] = median  # Replace with median\n",
    "        \n",
    "            \n",
    "    return data_filtered\n",
    "\n",
    "def customFilter(data, window_size,mu, medianFilter=False):\n",
    "    filtered_data = pd.DataFrame()\n",
    "    filtered_data = data.apply(lambda x: hampel(x, window_size=window_size, n_sigma=mu).filtered_data, axis=0)\n",
    "\n",
    "    if medianFilter:\n",
    "        filtered_data = rolling_median_filter(np.array(filtered_data))\n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "def true_label_finetune(model, X_train, y_train, X_test, y_test,\n",
    "                    n_epochs=2, batch_size=128, lookback = -1):\n",
    "\n",
    "    #  combine\n",
    "    X_train_combined = np.concatenate([X_train, X_test], axis=0)\n",
    "    y_train_combined = np.concatenate([y_train, y_test], axis=0)\n",
    "\n",
    "    # train using true-labels\n",
    "    if lookback != -1:\n",
    "        ft_model = train(copy.deepcopy(model), X_train_combined, y_train_combined, n_epochs=n_epochs,\n",
    "                         batch_size=batch_size, lookback=lookback, silent=True)\n",
    "    else:    \n",
    "        ft_model = train(copy.deepcopy(model), X_train_combined, y_train_combined, n_epochs=n_epochs,\n",
    "                         batch_size=batch_size, silent=True)    \n",
    "    return ft_model\n",
    " \n",
    "def hybrid_model_finetune(model1, model2, X_train, y_train, X_unlabeled, model1_preds, model2_preds, val_df,\n",
    "                          n_epochs=2, batch_size=128, lookback=0, cycle=-1):\n",
    "          \n",
    "    # Assign truth values based on the \n",
    "    # opposite models predictions and \n",
    "    # concat with training data\n",
    "    y_psuedo_label_for_model1 = model2_preds\n",
    "    y_psuedo_label_for_model2 = model1_preds\n",
    "    X_train_combined = np.concatenate([X_train, X_unlabeled[lookback:]], axis=0)\n",
    "    y_train_combined = np.concatenate([y_train, y_psuedo_label_for_model1], axis=0)\n",
    "\n",
    "\n",
    "    # finetune model1 (feedforward)\n",
    "    ft_model1 = train(copy.deepcopy(model1), X_train_combined, y_train_combined, n_epochs=n_epochs,\\\n",
    "                             batch_size=batch_size, silent=True)\n",
    "\n",
    "    # finetune model2 (LSTM)\n",
    "    X_train_combined = np.concatenate([X_train, X_unlabeled], axis=0)\n",
    "    y_train_combined = np.concatenate([y_train, y_psuedo_label_for_model2], axis=0)\n",
    "    ft_model2 = train(copy.deepcopy(model2), X_train_combined, y_train_combined, n_epochs=n_epochs,\\\n",
    "                             batch_size=batch_size, lookback=lookback, silent=True)\n",
    "\n",
    "\n",
    "    # if the test set has a greater cycle life than anything\n",
    "    # in the validation set, use the max cycle in the validation\n",
    "    max_cycle = val_df['c'].max()\n",
    "    cycle = min(cycle, max_cycle)\n",
    "    \n",
    "    # obtain target cycle from validation; note: currently only supports single cycle\n",
    "    mask = (val_df['c'] == cycle)\n",
    "    X_val_subset = val_df.loc[mask, features]\n",
    "    y_val_subset = val_df.loc[mask, [label]]\n",
    "\n",
    "    # Filter and transform validation subset\n",
    "    X_val_subset = customFilter(X_val_subset, 100, 2.5)\n",
    "    X_val_subset = StandardScaler().fit_transform(X_val_subset)\n",
    "    y_val_subset = y_val_subset.to_numpy()\n",
    "    lstm_X_test_cycle, _ = lstm_transform_data(X_val_subset, [], lookback)\n",
    "    \n",
    "    # predict with pre-fine-tuned models                 \n",
    "    y_pred_model1 = model1(torch.from_numpy(X_val_subset).to(device).float())\n",
    "    y_pred_model2 = model2(torch.from_numpy(lstm_X_test_cycle).to(device).float())\n",
    "    y_pred = (y_pred_model2.cpu().detach().numpy()[:, -1, :] + y_pred_model1.cpu().detach().numpy()[lookback:]) / 2\n",
    "\n",
    "    # output smoothing\n",
    "    y_pred = customFilter(pd.DataFrame(y_pred), 100, 2.5, medianFilter=True)\n",
    "\n",
    "    # predict with fine-tuned models\n",
    "    ft_y_pred_model1 = ft_model1(torch.from_numpy(X_val_subset).to(device).float())\n",
    "    ft_y_pred_model2 = ft_model2(torch.from_numpy(lstm_X_test_cycle).to(device).float())\n",
    "    ft_y_pred = (ft_y_pred_model2.cpu().detach().numpy()[:, -1, :] + ft_y_pred_model1.cpu().detach().numpy()[lookback:]) / 2\n",
    "\n",
    "    # output smoothing\n",
    "    ft_y_pred = customFilter(pd.DataFrame(ft_y_pred), 100, 2.5, medianFilter=True)\n",
    "\n",
    "    \"\"\"\n",
    "    Compare the error on the validation set before and after finetuning.\n",
    "    If the fine-tuning models perform better, return those. Otherwise,\n",
    "    \"rollback\" the changes and return the models from before finetuning.\n",
    "    \"\"\"\n",
    "    base_error = mean_squared_error(y_val_subset[lookback:], y_pred, squared=False)        \n",
    "    ft_error = mean_squared_error(y_val_subset[lookback:], ft_y_pred, squared=False)\n",
    "\n",
    "    if base_error < ft_error:\n",
    "        return model1, model2      \n",
    "    \n",
    "    elif base_error > ft_error:   \n",
    "        return ft_model1, ft_model2\n",
    "    \n",
    "    return ft_model1, ft_model2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4eb455b-5183-4915-bff0-558135ccff89",
   "metadata": {},
   "source": [
    "#### True-Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216f55e5-9dd0-404b-87bd-714b91543c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import csv\n",
    "\n",
    "raw_predictions = {}\n",
    "\n",
    "\"\"\"\n",
    "CL hyperparameters\n",
    "\n",
    "bs - batch size for finetuning\n",
    "e - Finetuning epochs\n",
    "cycle_interval - number of cyles between finetuning \n",
    "ratio - portion of target cycle's training data to use\n",
    "\"\"\"\n",
    "bs = 256\n",
    "e = 2\n",
    "cycle_interval = 1\n",
    "ratio = 0.6\n",
    "\n",
    "\"\"\"\n",
    "Hampel hyperparameters\n",
    "window - window size for input smoothing\n",
    "mu - hampel mu value for input smoothing\n",
    "\n",
    "\"\"\"\n",
    "window = 10\n",
    "mu = 2.5\n",
    "\n",
    "\n",
    "# fine-tuning\n",
    "C_finetuned_ff = copy.deepcopy(C_trained_ff).to(device)\n",
    "D_finetuned_ff = copy.deepcopy(D_trained_ff).to(device)\n",
    "C_finetuned_lstm = copy.deepcopy(C_trained_lstm).to(device)\n",
    "D_finetuned_lstm = copy.deepcopy(D_trained_lstm).to(device)\n",
    "\n",
    "\n",
    "# baseline\n",
    "C_trained_ff.to(device)\n",
    "D_trained_ff.to(device)\n",
    "C_trained_lstm.to(device)\n",
    "D_trained_lstm.to(device)\n",
    "\n",
    "\n",
    "C_max_cycle = C_test_df['c'].max()\n",
    "D_max_cycle = D_test_df['c'].max()\n",
    "max_cycle = int(min(C_max_cycle, D_max_cycle))\n",
    "\n",
    "\n",
    "print(\"True-Label SoC% error per cycle\")\n",
    "print(\"*\" * 85) \n",
    "for cycle in range(1, max_cycle + 1, cycle_interval):\n",
    "    \n",
    "    # Charging: Extract target cycles from training set\n",
    "    C_mask = (C_train_df['c'] >= cycle) & (C_train_df['c'] < cycle + cycle_interval)\n",
    "    C_X_train_subset = C_train_df.loc[C_mask, features]\n",
    "    C_y_train_subset = C_train_df.loc[C_mask, [label]]\n",
    "    \n",
    "    # Discharging: Extract target cycles from training set\n",
    "    D_mask = (D_train_df['c'] >= cycle) & (D_train_df['c'] < cycle + cycle_interval)\n",
    "    D_X_train_subset = D_train_df.loc[D_mask, features]\n",
    "    D_y_train_subset = D_train_df.loc[D_mask, [label]]\n",
    "\n",
    "    # Apply Hampel filter and transform training data to NumPy arrays\n",
    "    C_X_train_subset = customFilter(C_X_train_subset,window,mu)\n",
    "    D_X_train_subset = customFilter(D_X_train_subset,window,mu)\n",
    "    C_X_train_subset = StandardScaler().fit_transform(C_X_train_subset)\n",
    "    C_y_train_subset = C_y_train_subset.to_numpy()\n",
    "    D_X_train_subset = StandardScaler().fit_transform(D_X_train_subset)\n",
    "    D_y_train_subset = D_y_train_subset.to_numpy()\n",
    "\n",
    "    \n",
    "    # Only use 60% of the cycle's train data based\n",
    "    # on HP-tuning with true-label finetuning\n",
    "    cutoff = int(len(C_X_train_subset) * ratio)\n",
    "    C_X_train_subset =  C_X_train_subset[:cutoff]\n",
    "    C_y_train_subset = C_y_train_subset[:cutoff]\n",
    "    D_X_train_subset =  D_X_train_subset[:cutoff]\n",
    "    D_y_train_subset = D_y_train_subset[:cutoff]\n",
    "\n",
    "\n",
    "    # Charging: Extract target cycles from test set\n",
    "    C_mask = (C_test_df['c'] >= cycle) & (C_test_df['c'] < cycle + cycle_interval)\n",
    "    C_X_test_cycle = C_test_df.loc[C_mask, features]\n",
    "    C_y_test_cycle = C_test_df.loc[C_mask, [label]]\n",
    "    \n",
    "    # Discharging: Extract target cycles from test set\n",
    "    D_mask = (D_test_df['c'] >= cycle) & (D_test_df['c'] < cycle + cycle_interval)\n",
    "    D_X_test_cycle = D_test_df.loc[D_mask, features]\n",
    "    D_y_test_cycle = D_test_df.loc[D_mask, [label]]\n",
    "\n",
    "    # Get non-filtered version for baseline (models not using CL)\n",
    "    nf_C_X_test_cycle = copy.deepcopy(C_X_test_cycle)\n",
    "    nf_D_X_test_cycle = copy.deepcopy(D_X_test_cycle)\n",
    "    nf_C_X_test_cycle = StandardScaler().fit_transform(nf_C_X_test_cycle)\n",
    "    nf_D_X_test_cycle = StandardScaler().fit_transform(nf_D_X_test_cycle)\n",
    "    \n",
    "    # Apply Hampel filter and transform testing data to NumPy arrays\n",
    "    C_X_test_cycle = customFilter(C_X_test_cycle,window,mu)\n",
    "    D_X_test_cycle = customFilter(D_X_test_cycle,window,mu)\n",
    "    C_X_test_cycle = StandardScaler().fit_transform(C_X_test_cycle)\n",
    "    C_y_test_cycle = C_y_test_cycle.to_numpy()\n",
    "    D_X_test_cycle = StandardScaler().fit_transform(D_X_test_cycle)\n",
    "    D_y_test_cycle = D_y_test_cycle.to_numpy()\n",
    "\n",
    "    \n",
    "    # Set the baseline models to evaluation mode\n",
    "    C_trained_ff.eval()\n",
    "    D_trained_ff.eval()\n",
    "    C_trained_lstm.eval()\n",
    "    D_trained_lstm.eval()\n",
    "    \n",
    "    # Set the fine-tuning models to evaluation mode\n",
    "    C_finetuned_ff.eval() \n",
    "    D_finetuned_ff.eval()\n",
    "    C_finetuned_lstm.eval()\n",
    "    D_finetuned_lstm.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "         # Predict with base models\n",
    "        C_y_pred_base_ff = C_trained_ff(torch.from_numpy(nf_C_X_test_cycle).to(device).float())\n",
    "        D_y_pred_base_ff = D_trained_ff(torch.from_numpy(nf_D_X_test_cycle).to(device).float())\n",
    "        \n",
    "        nf_lstm_C_X_test_cycle, _ = lstm_transform_data(nf_C_X_test_cycle, [], C_window_size)\n",
    "        nf_lstm_D_X_test_cycle, _ = lstm_transform_data(nf_D_X_test_cycle, [], D_window_size)\n",
    "        C_y_pred_base_lstm = C_trained_lstm(torch.from_numpy(nf_lstm_C_X_test_cycle).to(device).float())\n",
    "        D_y_pred_base_lstm = D_trained_lstm(torch.from_numpy(nf_lstm_D_X_test_cycle).to(device).float())\n",
    "\n",
    "        print(C_y_pred_base_lstm.shape)\n",
    "        \n",
    "        # Predict with finetuned models \n",
    "        C_y_pred_ft_ff = C_finetuned_ff(torch.from_numpy(C_X_test_cycle).to(device).float())\n",
    "        D_y_pred_ft_ff = D_finetuned_ff(torch.from_numpy(D_X_test_cycle).to(device).float())\n",
    "\n",
    "        lstm_C_X_test_cycle, _ = lstm_transform_data(C_X_test_cycle, [], C_window_size)\n",
    "        lstm_D_X_test_cycle, _ = lstm_transform_data(D_X_test_cycle, [], D_window_size)\n",
    "        C_y_pred_ft_lstm = C_finetuned_lstm(torch.from_numpy(lstm_C_X_test_cycle).to(device).float())\n",
    "        D_y_pred_ft_lstm = D_finetuned_lstm(torch.from_numpy(lstm_D_X_test_cycle).to(device).float())\n",
    "\n",
    "    # average predictions (hybrid model approach)\n",
    "    C_y_pred_ft_hybrid = (C_y_pred_ft_lstm.cpu().detach().numpy()[:, -1, :] + C_y_pred_ft_ff.cpu().detach().numpy()[C_window_size:]) / 2\n",
    "    D_y_pred_ft_hybrid = (D_y_pred_ft_lstm.cpu().detach().numpy()[:, -1, :] + D_y_pred_ft_ff.cpu().detach().numpy()[D_window_size:]) / 2 \n",
    "    \n",
    "\n",
    "    # Apply Hampel and median filter to individual model outputs\n",
    "    C_ff = customFilter(pd.DataFrame(C_y_pred_ft_ff.cpu().detach().numpy()), 100, mu, medianFilter=True)\n",
    "    D_ff = customFilter(pd.DataFrame(D_y_pred_ft_ff.cpu().detach().numpy()), 100, mu, medianFilter=True)\n",
    "    C_lstm = customFilter(pd.DataFrame(C_y_pred_ft_lstm.cpu().detach().numpy()[:, -1, :]), 100, mu, medianFilter=True)\n",
    "    D_lstm = customFilter(pd.DataFrame(D_y_pred_ft_lstm.cpu().detach().numpy()[:, -1, :]), 100, mu, medianFilter=True)\n",
    "\n",
    "    # Apply Hampel and median filter to hybrid outputs\n",
    "    C_y_pred_ft_hybrid = customFilter(pd.DataFrame(C_y_pred_ft_hybrid), 100, mu, medianFilter=True)\n",
    "    D_y_pred_ft_hybrid = customFilter(pd.DataFrame(D_y_pred_ft_hybrid), 100, mu, medianFilter=True)\n",
    "     \n",
    "\n",
    "    \n",
    "    # # perform true-label fine-tuning\n",
    "    C_finetuned_ff = true_label_finetune(copy.deepcopy(C_finetuned_ff).to(device),\n",
    "                                         C_X_train_subset, C_y_train_subset, C_X_test_cycle, C_y_test_cycle, \n",
    "                                         n_epochs=e, batch_size=bs)     \n",
    "                            \n",
    "    D_finetuned_ff = true_label_finetune(copy.deepcopy(D_finetuned_ff).to(device),\n",
    "                                         D_X_train_subset, D_y_train_subset, D_X_test_cycle, D_y_test_cycle,\n",
    "                                         n_epochs=e, batch_size=bs)\n",
    "    \n",
    "    \n",
    "    C_finetuned_lstm = true_label_finetune(copy.deepcopy(C_finetuned_lstm).to(device),\n",
    "                                           C_X_train_subset, C_y_train_subset, C_X_test_cycle, C_y_test_cycle, \n",
    "                                           n_epochs=e, batch_size=bs, lookback=C_window_size)\n",
    "    \n",
    "    D_finetuned_lstm = true_label_finetune(copy.deepcopy(D_finetuned_lstm).to(device),\n",
    "                                           D_X_train_subset, D_y_train_subset, D_X_test_cycle, D_y_test_cycle, \n",
    "                                           n_epochs=e, batch_size=bs, lookback=D_window_size)     \n",
    "    \n",
    "    C_y_pred_base_hybrid = (C_y_pred_base_lstm.cpu().detach().numpy()[:, -1, :] + C_y_pred_base_ff.cpu().detach().numpy()[C_window_size:]) / 2\n",
    "    D_y_pred_base_hybrid = (D_y_pred_base_lstm.cpu().detach().numpy()[:, -1, :] + D_y_pred_base_ff.cpu().detach().numpy()[D_window_size:]) / 2 \n",
    "    \n",
    "    print(f\"{cycle} --\",  \n",
    "          \"FF:\", \n",
    "          round(100 * mean_squared_error(list(C_y_test_cycle[C_window_size:] / 1.1) +  list(D_y_test_cycle[D_window_size:]/ 1.1) ,list(C_y_pred_base_ff.cpu().detach().numpy()[C_window_size:] / 1.1) + list(D_y_pred_base_ff.cpu().detach().numpy()[D_window_size:] / 1.1), squared=False),2),\n",
    "          \"LSTM:\", \n",
    "          round(100 * mean_squared_error(list(C_y_test_cycle[C_window_size:]/ 1.1) +  list(D_y_test_cycle[D_window_size:] / 1.1) ,list(C_y_pred_base_lstm.cpu().detach().numpy()[:, -1, :] / 1.1) + list(D_y_pred_base_lstm.cpu().detach().numpy()[:, -1, :] / 1.1), squared=False),2),\n",
    "         \"Hybrid:\", \n",
    "          round(100 * mean_squared_error(list(C_y_test_cycle[C_window_size:] / 1.1) +  list(D_y_test_cycle[D_window_size:] / 1.1) ,list(C_y_pred_base_hybrid / 1.1) + list(D_y_pred_base_hybrid / 1.1), squared=False),2),\n",
    "          \n",
    "          \"FT-FF:\", \n",
    "          round(100 * mean_squared_error(list(C_y_test_cycle[C_window_size:] / 1.1) +  list(D_y_test_cycle[D_window_size:]/ 1.1) ,list(C_ff[C_window_size:] / 1.1) + list(D_ff[D_window_size:] / 1.1), squared=False),2),\n",
    "          \"FT-LSTM:\", \n",
    "          round(100 * mean_squared_error(list(C_y_test_cycle[C_window_size:]/ 1.1) +  list(D_y_test_cycle[D_window_size:] / 1.1) ,list(C_lstm/ 1.1) + list(D_lstm / 1.1), squared=False),2),\n",
    "         \"FT-Hybrid:\", \n",
    "          round(100 * mean_squared_error(list(C_y_test_cycle[C_window_size:] / 1.1) +  list(D_y_test_cycle[D_window_size:] / 1.1) ,list(C_y_pred_ft_hybrid / 1.1) + list(D_y_pred_ft_hybrid / 1.1), squared=False),2),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de388062-4e93-404d-a01a-9f8d216e4a63",
   "metadata": {},
   "source": [
    "#### Psuedo-Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bf9daa-82ea-48ad-83f7-5776188ead81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import csv\n",
    "\n",
    "raw_predictions = {}\n",
    "\n",
    "\"\"\"\n",
    "CL hyperparameters\n",
    "\n",
    "bs - batch size for finetuning\n",
    "e - Finetuning epochs\n",
    "cycle_interval - number of cyles between finetuning \n",
    "ratio - portion of target cycle's training data to use\n",
    "\"\"\"\n",
    "bs = 256\n",
    "e = 2\n",
    "cycle_interval = 1\n",
    "ratio = 0.6\n",
    "\n",
    "\"\"\"\n",
    "Hampel hyperparameters\n",
    "window - window size for input smoothing\n",
    "mu - hampel mu value for input smoothing\n",
    "\n",
    "\"\"\"\n",
    "window = 10\n",
    "mu = 2.5\n",
    "\n",
    "\n",
    "# hybrid fine-tuning\n",
    "C_finetuned_ff = copy.deepcopy(C_trained_ff).to(device)\n",
    "D_finetuned_ff = copy.deepcopy(D_trained_ff).to(device)\n",
    "C_finetuned_lstm = copy.deepcopy(C_trained_lstm).to(device)\n",
    "D_finetuned_lstm = copy.deepcopy(D_trained_lstm).to(device)\n",
    "\n",
    "\n",
    "# baseline\n",
    "C_trained_ff.to(device)\n",
    "D_trained_ff.to(device)\n",
    "C_trained_lstm.to(device)\n",
    "D_trained_lstm.to(device)\n",
    "\n",
    "\n",
    "C_max_cycle = C_test_df['c'].max()\n",
    "D_max_cycle = D_test_df['c'].max()\n",
    "max_cycle = int(min(C_max_cycle, D_max_cycle))\n",
    "\n",
    "\n",
    "print(\"Psuedo-label SoC% error per cycle\")\n",
    "print(\"*\" * 65) \n",
    "for cycle in range(1, max_cycle + 1, cycle_interval):\n",
    "    \n",
    "    # Charging: Extract target cycles from training set\n",
    "    C_mask = (C_train_df['c'] >= cycle) & (C_train_df['c'] < cycle + cycle_interval)\n",
    "    C_X_train_subset = C_train_df.loc[C_mask, features]\n",
    "    C_y_train_subset = C_train_df.loc[C_mask, [label]]\n",
    "    \n",
    "    # Discharging: Extract target cycles from training set\n",
    "    D_mask = (D_train_df['c'] >= cycle) & (D_train_df['c'] < cycle + cycle_interval)\n",
    "    D_X_train_subset = D_train_df.loc[D_mask, features]\n",
    "    D_y_train_subset = D_train_df.loc[D_mask, [label]]\n",
    "\n",
    "    # Apply Hampel filter and transform training data to NumPy arrays\n",
    "    C_X_train_subset = customFilter(C_X_train_subset,window,mu)\n",
    "    D_X_train_subset = customFilter(D_X_train_subset,window,mu)\n",
    "    C_X_train_subset = StandardScaler().fit_transform(C_X_train_subset)\n",
    "    C_y_train_subset = C_y_train_subset.to_numpy()\n",
    "    D_X_train_subset = StandardScaler().fit_transform(D_X_train_subset)\n",
    "    D_y_train_subset = D_y_train_subset.to_numpy()\n",
    "\n",
    "    \n",
    "    # Only use 60% of the cycle's train data based\n",
    "    # on HP-tuning with true-label finetuning\n",
    "    cutoff = int(len(C_X_train_subset) * ratio)\n",
    "    C_X_train_subset =  C_X_train_subset[:cutoff]\n",
    "    C_y_train_subset = C_y_train_subset[:cutoff]\n",
    "    D_X_train_subset =  D_X_train_subset[:cutoff]\n",
    "    D_y_train_subset = D_y_train_subset[:cutoff]\n",
    "\n",
    "\n",
    "    # Charging: Extract target cycles from test set\n",
    "    C_mask = (C_test_df['c'] >= cycle) & (C_test_df['c'] < cycle + cycle_interval)\n",
    "    C_X_test_cycle = C_test_df.loc[C_mask, features]\n",
    "    C_y_test_cycle = C_test_df.loc[C_mask, [label]]\n",
    "    \n",
    "    # Discharging: Extract target cycles from test set\n",
    "    D_mask = (D_test_df['c'] >= cycle) & (D_test_df['c'] < cycle + cycle_interval)\n",
    "    D_X_test_cycle = D_test_df.loc[D_mask, features]\n",
    "    D_y_test_cycle = D_test_df.loc[D_mask, [label]]\n",
    "\n",
    "    # Get non-filtered version for baseline (models not using CL)\n",
    "    nf_C_X_test_cycle = copy.deepcopy(C_X_test_cycle)\n",
    "    nf_D_X_test_cycle = copy.deepcopy(D_X_test_cycle)\n",
    "    nf_C_X_test_cycle = StandardScaler().fit_transform(nf_C_X_test_cycle)\n",
    "    nf_D_X_test_cycle = StandardScaler().fit_transform(nf_D_X_test_cycle)\n",
    "    \n",
    "    # Apply Hampel filter and transform testing data to NumPy arrays\n",
    "    C_X_test_cycle = customFilter(C_X_test_cycle,window,mu)\n",
    "    D_X_test_cycle = customFilter(D_X_test_cycle,window,mu)\n",
    "    C_X_test_cycle = StandardScaler().fit_transform(C_X_test_cycle)\n",
    "    C_y_test_cycle = C_y_test_cycle.to_numpy()\n",
    "    D_X_test_cycle = StandardScaler().fit_transform(D_X_test_cycle)\n",
    "    D_y_test_cycle = D_y_test_cycle.to_numpy()\n",
    "\n",
    "    \n",
    "    # Set the baseline models to evaluation mode\n",
    "    C_trained_ff.eval()\n",
    "    D_trained_ff.eval()\n",
    "    C_trained_lstm.eval()\n",
    "    D_trained_lstm.eval()\n",
    "\n",
    "    \n",
    "    # Set the CL-WR models to evaluation mode\n",
    "    C_finetuned_ff.eval() \n",
    "    D_finetuned_ff.eval()\n",
    "    C_finetuned_lstm.eval()\n",
    "    D_finetuned_lstm.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "         # Predict with base models\n",
    "        C_y_pred_base_ff = C_trained_ff(torch.from_numpy(nf_C_X_test_cycle).to(device).float())\n",
    "        D_y_pred_base_ff = D_trained_ff(torch.from_numpy(nf_D_X_test_cycle).to(device).float())\n",
    "        \n",
    "        nf_lstm_C_X_test_cycle, _ = lstm_transform_data(nf_C_X_test_cycle, [], C_window_size)\n",
    "        nf_lstm_D_X_test_cycle, _ = lstm_transform_data(nf_D_X_test_cycle, [], D_window_size)\n",
    "        C_y_pred_base_lstm = C_trained_lstm(torch.from_numpy(nf_lstm_C_X_test_cycle).to(device).float())\n",
    "        D_y_pred_base_lstm = D_trained_lstm(torch.from_numpy(nf_lstm_D_X_test_cycle).to(device).float())\n",
    "  \n",
    "        # Predict with models finetuned using CL-WR\n",
    "        C_y_pred_ft_ff = C_finetuned_ff(torch.from_numpy(C_X_test_cycle).to(device).float())\n",
    "        D_y_pred_ft_ff = D_finetuned_ff(torch.from_numpy(D_X_test_cycle).to(device).float())\n",
    "\n",
    "        lstm_C_X_test_cycle, _ = lstm_transform_data(C_X_test_cycle, [], C_window_size)\n",
    "        lstm_D_X_test_cycle, _ = lstm_transform_data(D_X_test_cycle, [], D_window_size)\n",
    "        C_y_pred_ft_lstm = C_finetuned_lstm(torch.from_numpy(lstm_C_X_test_cycle).to(device).float())\n",
    "        D_y_pred_ft_lstm = D_finetuned_lstm(torch.from_numpy(lstm_D_X_test_cycle).to(device).float())\n",
    "\n",
    "    # average predictions (hybrid model approach)\n",
    "    C_y_pred_ft_hybrid = (C_y_pred_ft_lstm.cpu().detach().numpy()[:, -1, :] + C_y_pred_ft_ff.cpu().detach().numpy()[C_window_size:]) / 2\n",
    "    D_y_pred_ft_hybrid = (D_y_pred_ft_lstm.cpu().detach().numpy()[:, -1, :] + D_y_pred_ft_ff.cpu().detach().numpy()[D_window_size:]) / 2 \n",
    "    \n",
    "\n",
    "    # Apply Hampel and median filter to hybrid outputs\n",
    "    C_y_pred_ft_hybrid = customFilter(pd.DataFrame(C_y_pred_ft_hybrid), 100, mu, medianFilter=True)\n",
    "    D_y_pred_ft_hybrid = customFilter(pd.DataFrame(D_y_pred_ft_hybrid), 100, mu, medianFilter=True)\n",
    "     \n",
    "    # Apply Hampel and median filter to individual model outputs for dual-model CL-WR\n",
    "    C_ff = customFilter(pd.DataFrame(C_y_pred_ft_ff.cpu().detach().numpy()), 100, mu, medianFilter=True)\n",
    "    D_ff = customFilter(pd.DataFrame(D_y_pred_ft_ff.cpu().detach().numpy()), 100, mu, medianFilter=True)\n",
    "    C_lstm = customFilter(pd.DataFrame(C_y_pred_ft_lstm.cpu().detach().numpy()[:, -1, :]), 100, mu, medianFilter=True)\n",
    "    D_lstm = customFilter(pd.DataFrame(D_y_pred_ft_lstm.cpu().detach().numpy()[:, -1, :]), 100, mu, medianFilter=True)\n",
    " \n",
    "    # perform CL-WR\n",
    "    C_finetuned_ff, C_finetuned_lstm = hybrid_model_finetune(copy.deepcopy(C_finetuned_ff).to(device), copy.deepcopy(C_finetuned_lstm).to(device),\n",
    "                                                             C_X_train_subset, C_y_train_subset, C_X_test_cycle, C_ff, C_lstm, C_val_df,\n",
    "                                                             n_epochs=e, batch_size=bs, lookback=C_window_size, cycle=cycle)\n",
    "    \n",
    "    D_finetuned_ff, D_finetuned_lstm = hybrid_model_finetune(copy.deepcopy(D_finetuned_ff).to(device), copy.deepcopy(D_finetuned_lstm).to(device),\n",
    "                                                             D_X_train_subset, D_y_train_subset, D_X_test_cycle, D_ff, D_lstm, D_val_df,\n",
    "                                                             n_epochs=e, batch_size=bs,lookback=D_window_size, cycle=cycle)\n",
    "          \n",
    "    C_y_pred_base_hybrid = (C_y_pred_base_lstm.cpu().detach().numpy()[:, -1, :] + C_y_pred_base_ff.cpu().detach().numpy()[C_window_size:]) / 2\n",
    "    D_y_pred_base_hybrid = (D_y_pred_base_lstm.cpu().detach().numpy()[:, -1, :] + D_y_pred_base_ff.cpu().detach().numpy()[D_window_size:]) / 2 \n",
    "    \n",
    "    print(f\"{cycle} --\",  \n",
    "          \"FF:\", \n",
    "          round(100 * mean_squared_error(list(C_y_test_cycle[C_window_size:] / 1.1) +  list(D_y_test_cycle[D_window_size:]/ 1.1) ,list(C_y_pred_base_ff.cpu().detach().numpy()[C_window_size:] / 1.1) + list(D_y_pred_base_ff.cpu().detach().numpy()[D_window_size:] / 1.1), squared=False),2),\n",
    "          \"LSTM:\", \n",
    "          round(100 * mean_squared_error(list(C_y_test_cycle[C_window_size:]/ 1.1) +  list(D_y_test_cycle[D_window_size:] / 1.1) ,list(C_y_pred_base_lstm.cpu().detach().numpy()[:, -1, :] / 1.1) + list(D_y_pred_base_lstm.cpu().detach().numpy()[:, -1, :] / 1.1), squared=False),2),\n",
    "         \"Hybrid:\", \n",
    "          round(100 * mean_squared_error(list(C_y_test_cycle[C_window_size:] / 1.1) +  list(D_y_test_cycle[D_window_size:] / 1.1) ,list(C_y_pred_base_hybrid / 1.1) + list(D_y_pred_base_hybrid / 1.1), squared=False),2),\n",
    "         \"CL-WR:\",\n",
    "          round(100 * mean_squared_error(list(C_y_test_cycle[C_window_size:] / 1.1) +  list(D_y_test_cycle[D_window_size:] / 1.1) ,list(C_y_pred_ft_hybrid / 1.1) + list(D_y_pred_ft_hybrid / 1.1), squared=False),2)\n",
    "    )\n",
    "    \n",
    "    \"\"\"\n",
    "    Prediction array format:\n",
    "    \n",
    "        truth_charging, truth_discharging, \n",
    "        feedforward_charging, feedforward_discharging,\n",
    "        lstm_charging, lstm_discharging,\n",
    "        hybrid_charging, hybrid_discharging,\n",
    "        CL_WR_charging, CL_WR_discharging\n",
    "    \"\"\"\n",
    "    raw_items = [\n",
    "        C_y_test_cycle[C_window_size:], D_y_test_cycle[D_window_size:], \n",
    "        C_y_pred_base_ff.cpu().detach().numpy()[C_window_size:], D_y_pred_base_ff.cpu().detach().numpy()[D_window_size:],\n",
    "        C_y_pred_base_lstm.cpu().detach().numpy()[:, -1, :], D_y_pred_base_lstm.cpu().detach().numpy()[:, -1, :],\n",
    "        C_y_pred_base_hybrid, D_y_pred_base_hybrid,\n",
    "        C_y_pred_ft_hybrid, D_y_pred_ft_hybrid\n",
    "    ]\n",
    "    processed_items = []\n",
    "    for item in raw_items:\n",
    "        processed_items.append(item.flatten().tolist())\n",
    "    raw_predictions[cycle] = processed_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69afb60c-8d12-4ee8-82fd-1353dda0e4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Optionally, save the predictions for later analysis\n",
    "with open(\"raw_psuedo_label_predictions.json\", \"w\") as f:\n",
    "    json.dump(raw_predictions, f, indent=4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1551027-2b27-44bf-9604-cc5e3c44588d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
